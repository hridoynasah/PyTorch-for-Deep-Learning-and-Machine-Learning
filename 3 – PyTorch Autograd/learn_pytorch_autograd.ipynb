{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcd0b6c",
   "metadata": {},
   "source": [
    "##### **Autograd:**\n",
    "We need **autograd** because it automatically computes gradients (derivatives), which are essential for training neural networks using backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b8bc7",
   "metadata": {},
   "source": [
    "**General Steps of Using Autograd in PyTorch**\n",
    "\n",
    "1. **Create tensors with gradient tracking**\n",
    "\n",
    "   ```python\n",
    "   x = torch.tensor(value, requires_grad=True)\n",
    "   ```\n",
    "\n",
    "   * Tensors with `requires_grad=True` are tracked by autograd.\n",
    "\n",
    "2. **Define the computation / forward pass**\n",
    "\n",
    "   ```python\n",
    "   y = some_function(x)\n",
    "   ```\n",
    "\n",
    "   * PyTorch builds a **computational graph** connecting inputs to outputs.\n",
    "\n",
    "3. **Compute gradients with `.backward()`**\n",
    "\n",
    "   ```python\n",
    "   y.backward()\n",
    "   ```\n",
    "\n",
    "   * Applies **chain rule** automatically to compute derivatives.\n",
    "   * For scalar outputs, you can call `.backward()` directly.\n",
    "\n",
    "4. **Access the gradients**\n",
    "\n",
    "   ```python\n",
    "   x.grad\n",
    "   ```\n",
    "\n",
    "   * Gradients of the output with respect to each input are stored in `.grad`.\n",
    "\n",
    "5. **Use gradients for optimization (optional)**\n",
    "\n",
    "   * Typically, update model parameters using an optimizer:\n",
    "\n",
    "     ```python\n",
    "     x = x - learning_rate * x.grad\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6db32f",
   "metadata": {},
   "source": [
    "* Example without using autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0765591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# diff of x^2\n",
    "def dy_dx(x):\n",
    "  return 2*x\n",
    "\n",
    "print(dy_dx(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab8ea6",
   "metadata": {},
   "source": [
    "* Using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f189038",
   "metadata": {},
   "source": [
    "* Example without using autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75955374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.661275842587077"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diff of y = x^2 & z = sin(y)\n",
    "import math\n",
    "def dz_dx(x):\n",
    "    return 2 * x * math.cos(x**2)\n",
    "dz_dx(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc674d02",
   "metadata": {},
   "source": [
    "* Using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19add8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.6613) None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hridoy\\AppData\\Local\\Temp\\ipykernel_8044\\1829653141.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(x.grad, y.grad)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(4.0, requires_grad = True)\n",
    "y = x ** 2\n",
    "z = torch.sin(y)\n",
    "z.backward() # this is performing diff\n",
    "print(x.grad, y.grad)\n",
    "\n",
    "# we can only get the gradient of the leaf which is x\n",
    "# z -> sin -> y -> sqrt -> x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4836ce",
   "metadata": {},
   "source": [
    "* Manual process `BCE` loss backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e09d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 4, 7, 7, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([2, 1, 4, 9, 8, 6, 7])\n",
    "x = torch.clamp(x, min=3, max=7)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8870bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(6.7) # input feature\n",
    "y = torch.tensor(0.0) # True label\n",
    "\n",
    "w = torch.tensor(1.0)\n",
    "b = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42b571c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(prediction, target):\n",
    "    epsilon = 1e-8\n",
    "    prediction = torch.clamp(prediction, epsilon, 1-epsilon)\n",
    "    return -(target*torch.log(prediction) + (1-target)*torch.log(1-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c8ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7012)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "z = w * x + b\n",
    "y_pred = torch.sigmoid(z)\n",
    "loss = binary_cross_entropy(y_pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f23a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dypred = (y_pred - y)/(y_pred *(1 - y_pred))\n",
    "dypred_dz = y_pred *(1 - y_pred)\n",
    "dz_dw = x\n",
    "dz_db = 1\n",
    "dL_dw = dl_dypred * dypred_dz * dz_dw\n",
    "dL_db = dl_dypred * dypred_dz * dz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "195439d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b3bf1",
   "metadata": {},
   "source": [
    "* Using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e6effb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(6.7)\n",
    "y = torch.tensor(0.0)\n",
    "\n",
    "w_new = torch.tensor(1.0, requires_grad=True)\n",
    "b_new = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d9857a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9988, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = w_new * z + b_new\n",
    "y_pred = torch.sigmoid(z)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5993a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7012, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = binary_cross_entropy(y_pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c749d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918) tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w_new.grad, b_new.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13089aa9",
   "metadata": {},
   "source": [
    "* Using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61840d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = (x**2).mean()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87464743",
   "metadata": {},
   "source": [
    "* Clearing gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f98b9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2423ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9f8e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b003749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# diff newly every time after running forwarpass again and again\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97859665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Clearing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87664f33",
   "metadata": {},
   "source": [
    "* When we don't need gradient tracking?  \n",
    "\n",
    "    In the time of macking predictions  \n",
    "\n",
    "    Options:  \n",
    "    \n",
    "    1 - `requires_grad_(False)`  \n",
    "    2 - `detach()`  \n",
    "    3 - `torch.no_grad()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2273270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using torch.no_grad()\n",
    "import torch\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Gradient tracking is off now\n",
    "with torch.no_grad():\n",
    "    y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9246d6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# backpropagation is not possible because gradient tracking is off\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# backpropagation is not possible because gradient tracking is off\n",
    "y.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
