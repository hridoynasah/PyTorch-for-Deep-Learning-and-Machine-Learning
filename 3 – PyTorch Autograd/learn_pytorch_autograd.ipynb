{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcd0b6c",
   "metadata": {},
   "source": [
    "##### **Autograd:**\n",
    "We need **autograd** because it automatically computes gradients (derivatives), which are essential for training neural networks using backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b8bc7",
   "metadata": {},
   "source": [
    "**General Steps of Using Autograd in PyTorch**\n",
    "\n",
    "1. **Create tensors with gradient tracking**\n",
    "\n",
    "   ```python\n",
    "   x = torch.tensor(value, requires_grad=True)\n",
    "   ```\n",
    "\n",
    "   * Tensors with `requires_grad=True` are tracked by autograd.\n",
    "\n",
    "2. **Define the computation / forward pass**\n",
    "\n",
    "   ```python\n",
    "   y = some_function(x)\n",
    "   ```\n",
    "\n",
    "   * PyTorch builds a **computational graph** connecting inputs to outputs.\n",
    "\n",
    "3. **Compute gradients with `.backward()`**\n",
    "\n",
    "   ```python\n",
    "   y.backward()\n",
    "   ```\n",
    "\n",
    "   * Applies **chain rule** automatically to compute derivatives.\n",
    "   * For scalar outputs, you can call `.backward()` directly.\n",
    "\n",
    "4. **Access the gradients**\n",
    "\n",
    "   ```python\n",
    "   x.grad\n",
    "   ```\n",
    "\n",
    "   * Gradients of the output with respect to each input are stored in `.grad`.\n",
    "\n",
    "5. **Use gradients for optimization (optional)**\n",
    "\n",
    "   * Typically, update model parameters using an optimizer:\n",
    "\n",
    "     ```python\n",
    "     x = x - learning_rate * x.grad\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6db32f",
   "metadata": {},
   "source": [
    "* Example without using autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0765591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# diff of x^2\n",
    "def dy_dx(x):\n",
    "  return 2*x\n",
    "\n",
    "print(dy_dx(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab8ea6",
   "metadata": {},
   "source": [
    "* Using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f189038",
   "metadata": {},
   "source": [
    "* Example without using autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75955374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.661275842587077"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diff of y = x^2 & z = sin(y)\n",
    "import math\n",
    "def dz_dx(x):\n",
    "    return 2 * x * math.cos(x**2)\n",
    "dz_dx(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc674d02",
   "metadata": {},
   "source": [
    "* Using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19add8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.6613) None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hridoy\\AppData\\Local\\Temp\\ipykernel_8044\\1829653141.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(x.grad, y.grad)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(4.0, requires_grad = True)\n",
    "y = x ** 2\n",
    "z = torch.sin(y)\n",
    "z.backward() # this is performing diff\n",
    "print(x.grad, y.grad)\n",
    "\n",
    "# we can only get the gradient of the leaf which is x\n",
    "# z -> sin -> y -> sqrt -> x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4836ce",
   "metadata": {},
   "source": [
    "* Manual process `BCE` loss backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e09d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 4, 7, 7, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([2, 1, 4, 9, 8, 6, 7])\n",
    "x = torch.clamp(x, min=3, max=7)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8870bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(6.7) # input feature\n",
    "y = torch.tensor(0.0) # True label\n",
    "\n",
    "w = torch.tensor(1.0)\n",
    "b = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42b571c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(prediction, target):\n",
    "    epsilon = 1e-8\n",
    "    prediction = torch.clamp(prediction, epsilon, 1-epsilon)\n",
    "    return -(target*torch.log(prediction) + (1-target)*torch.log(1-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c8ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7012)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "z = w * x + b\n",
    "y_pred = torch.sigmoid(z)\n",
    "loss = binary_cross_entropy(y_pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f23a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dypred = (y_pred - y)/(y_pred *(1 - y_pred))\n",
    "dypred_dz = y_pred *(1 - y_pred)\n",
    "dz_dw = x\n",
    "dz_db = 1\n",
    "dL_dw = dl_dypred * dypred_dz * dz_dw\n",
    "dL_db = dl_dypred * dypred_dz * dz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "195439d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b3bf1",
   "metadata": {},
   "source": [
    "* Using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e6effb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(6.7)\n",
    "y = torch.tensor(0.0)\n",
    "\n",
    "w_new = torch.tensor(1.0, requires_grad=True)\n",
    "b_new = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d9857a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9988, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = w_new * z + b_new\n",
    "y_pred = torch.sigmoid(z)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5993a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7012, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = binary_cross_entropy(y_pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c749d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918) tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w_new.grad, b_new.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13089aa9",
   "metadata": {},
   "source": [
    "* Using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61840d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = (x**2).mean()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87464743",
   "metadata": {},
   "source": [
    "* Clearing gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f98b9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2423ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9f8e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b003749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# diff newly every time after running forwarpass again and again\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97859665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Clearing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87664f33",
   "metadata": {},
   "source": [
    "* When we don't need gradient tracking?  \n",
    "\n",
    "In the time of macking predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
